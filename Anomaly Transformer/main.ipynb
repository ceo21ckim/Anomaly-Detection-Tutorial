{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn, optim \n",
    "import torch.nn.functional as F \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "from tqdm import tqdm \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os \n",
    "\n",
    "\n",
    "from model import * \n",
    "# from modeling import * \n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    win_size = 100 \n",
    "    input_c = 25\n",
    "    output_c = 25\n",
    "    num_layers = 3\n",
    "    num_epochs = 200\n",
    "    batch_size = 256 \n",
    "    num_workers = 0\n",
    "    lambda_ = 3\n",
    "    lr = 1e-4\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = get_loader(args, mode='train')\n",
    "# test_loader = get_loader(args, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSMSegLoader(object):\n",
    "    def __init__(self, data_path, win_size, step, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.step = step\n",
    "        self.win_size = win_size\n",
    "        self.scaler = StandardScaler()\n",
    "        data = pd.read_csv(data_path + '/data/train.csv')\n",
    "        data = data.values[:, 1:]\n",
    "\n",
    "        data = np.nan_to_num(data)\n",
    "\n",
    "        self.scaler.fit(data)\n",
    "        data = self.scaler.transform(data)\n",
    "        test_data = pd.read_csv(data_path + '/data/test.csv')\n",
    "\n",
    "        test_data = test_data.values[:, 1:]\n",
    "        test_data = np.nan_to_num(test_data)\n",
    "\n",
    "        self.test = self.scaler.transform(test_data)\n",
    "\n",
    "        self.train = data\n",
    "        self.val = self.test\n",
    "\n",
    "        self.test_labels = pd.read_csv(data_path + '/data/test_label.csv').values[:, 1:]\n",
    "\n",
    "        print(\"test:\", self.test.shape)\n",
    "        print(\"train:\", self.train.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of images in the object dataset.\n",
    "        \"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            return (self.train.shape[0] - self.win_size) // self.step + 1\n",
    "        elif (self.mode == 'val'):\n",
    "            return (self.val.shape[0] - self.win_size) // self.step + 1\n",
    "        elif (self.mode == 'test'):\n",
    "            return (self.test.shape[0] - self.win_size) // self.step + 1\n",
    "        else:\n",
    "            return (self.test.shape[0] - self.win_size) // self.win_size + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = index * self.step\n",
    "        if self.mode == \"train\":\n",
    "            return np.float32(self.train[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
    "        elif (self.mode == 'val'):\n",
    "            return np.float32(self.val[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
    "        elif (self.mode == 'test'):\n",
    "            return np.float32(self.test[index:index + self.win_size]), np.float32(\n",
    "                self.test_labels[index:index + self.win_size])\n",
    "        else:\n",
    "            return np.float32(self.test[\n",
    "                              index // self.step * self.win_size:index // self.step * self.win_size + self.win_size]), np.float32(\n",
    "                self.test_labels[index // self.step * self.win_size:index // self.step * self.win_size + self.win_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PSMSegLoader('.', args.win_size, 1, mode='train')\n",
    "test_set = PSMSegLoader('.', args.win_size, 1, mode='test')\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnomalyTransformer(\n",
    "    window_size = args.win_size, \n",
    "    enc_in = args.input_c, \n",
    "    c_out = args.output_c, \n",
    "    e_layers = args.num_layers\n",
    ").to(args.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "criterion = nn.MSELoss().to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate( model, valid_loader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    min_losses, max_losses = [], []\n",
    "    \n",
    "    for batch in valid_loader:\n",
    "        input_x, label = tuple(b.to(args.device) for b in batch)\n",
    "        \n",
    "        series_loss, prior_loss = 0.0, 0.0\n",
    "        \n",
    "        x_hat, series, prior, _ = model(input_x)\n",
    "        \n",
    "        for i in range(len(prior)):\n",
    "            series_loss += (torch.mean(F.kl_div(series[i], (\n",
    "                prior[i] / torch.unsqueeze(torch.sum(prior[i], dim=-1), dim=-1).repeat(1, 1, 1, args.win_size)).detach())) + torch.mean(\n",
    "                \n",
    "                F.kl_div(\n",
    "                    (prior[i] / torch.unsqueeze(torch.sum(prior[i], dim=-1), dim=-1).repeat(1, 1, 1, args.win_size)).detach(), series[i])\n",
    "            ))\n",
    "                \n",
    "            prior_loss += (torch.mean(\n",
    "                F.kl_div((prior[i] / torch.unsqueeze(torch.sum(prior[i], dim=-1), dim=-1).repeat(1, 1, 1, args.win_size)), \n",
    "                             series[i].detach())) + torch.mean(\n",
    "                F.kl_div(series[i].detach(), \n",
    "                             (prior[i] / torch.unsqueeze(torch.sum(prior[i], dim=-1), dim=-1).repeat(1, 1, 1, args.win_size)))))\n",
    "            \n",
    "        series_loss = series_loss / len(prior)\n",
    "        prior_loss = prior_loss / len(prior)\n",
    "        reconstruction_loss = criterion(x_hat, input_x)\n",
    "        \n",
    "        \n",
    "        max_loss = reconstruction_loss - args.lambda_ * prior_loss\n",
    "        min_loss = reconstruction_loss + args.lambda_ * series_loss \n",
    "    \n",
    "    return np.average(min_losses), np.average(max_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tqdm(range(args.num_epochs), desc='training...')\n",
    "for epoch in iterator:\n",
    "    min_best_loss = float('inf')\n",
    "    max_best_loss = float('inf')\n",
    "    \n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        train_losses = 0.0\n",
    "        input_x, label = tuple(b.to(args.device) for b in batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        input_x = input_x.float().to(args.device)\n",
    "        \n",
    "        x_hat, series, prior, _ = model(input_x) # (x_hat, series, prior, sigma)\n",
    "        \n",
    "        series_loss, prior_loss = 0.0, 0.0 \n",
    "        for i in range(len(prior)):\n",
    "            series_loss += (torch.mean(F.kl_div(series[i], (\n",
    "                prior[i] / torch.unsqueeze(torch.sum(prior[i], dim=-1), dim=-1).repeat(1, 1, 1, args.win_size)).detach())) + torch.mean(\n",
    "                \n",
    "                F.kl_div(\n",
    "                    (prior[i] / torch.unsqueeze(torch.sum(prior[i], dim=-1), dim=-1).repeat(1, 1, 1, args.win_size)).detach(), series[i])\n",
    "            ))\n",
    "                \n",
    "            prior_loss += (torch.mean(\n",
    "                F.kl_div((prior[i] / torch.unsqueeze(torch.sum(prior[i], dim=-1), dim=-1).repeat(1, 1, 1, args.win_size)), \n",
    "                             series[i].detach())) + torch.mean(\n",
    "                F.kl_div(series[i].detach(), \n",
    "                             (prior[i] / torch.unsqueeze(torch.sum(prior[i], dim=-1), dim=-1).repeat(1, 1, 1, args.win_size)))))\n",
    "                \n",
    "        series_loss = series_loss / len(prior)\n",
    "        prior_loss = prior_loss / len(prior)\n",
    "        reconstruction_loss = criterion(x_hat, input_x)\n",
    "\n",
    "        max_loss = reconstruction_loss - args.lambda_ * prior_loss\n",
    "        min_loss = reconstruction_loss + args.lambda_ * series_loss \n",
    "        \n",
    "        iterator.set_postfix({\n",
    "            'min_loss':min_loss.item(), \n",
    "            'max_loss':max_loss.item(), \n",
    "            'rec_loss':reconstruction_loss.item()})\n",
    "        \n",
    "        if (idx + 1 % 50) == 0 :\n",
    "            print(f'Epoch: [{epoch+1}/{args.num_epochs}]\\t iterator: [{idx+1}/{len(train_loader)}]')\n",
    "            print(f'max loss: {max_loss:.4f},\\tmin_loss: {min_loss:.4f},\\treconstruction loss: {reconstruction_loss:.4f}')\n",
    "        \n",
    "        # Minimax strategy\n",
    "        max_loss.backward(retain_graph=True)\n",
    "        min_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    valid_min_loss, valid_max_loss = evaluate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
